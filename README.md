# MapReduce на go

Map Reduce на go на примере задачи подсчета количества вхождений каждого слова. Во всех реализациях считаем, что слова состоят из символов алфавита и цифр.

Тут представлены 3 функции, реализующие подсчет слов:

1. Однопоточная программа на golang -- [single/main.go](single/main.go)
2. Чтение большого файла в параллель по N горутинам (здесь не учитывается, что слова могут быть "разорваны" в 2 соседних буфера) -- [mapReduceBigFile/main.go](mapReduceBigFile/main.go)
3. Чтение директории рекурсивно в параллель по N горутинам -- [mapReduceDir/main.go](mapReduceDir/main.go)

В последнем примере шаг reduce выполняется параллельно map, за счет чего происходит оптимизация по памяти (не нужно держать в памяти все карты для каждого файла). Так же, за счет этого можно настроить удобное число горутин, уменьшив при этом context switching, и, тем самым, еще дополнительно ускорив программу.
Изменение параметра N от единицы до 5-7 меняет скорость выполнения примерно в 2 раза (видимо, значительная часть времени уходит на дисковые операции чтения/записи). Дальнейшее увеличение количества горутин не приносит увеличения скорости. 

Для проверки так же написал на bash однострочники.

Список уникальных слов с частотой (сортировка по частоте):

```bash
cat words.txt | sed -E 's/[^[:alnum:]]/ /g' | tr -s ' ' '\n' | sort | uniq --count | sort -r -n
```

Список уникальных слов с частотой (сортировка по словам):

```bash
cat words.txt | sed -E 's/[^[:alnum:]]/ /g' | tr -s ' ' '\n' | sort | uniq --count | sed -E 's/^\s+//g' | awk '{print $$2 " " $$1; }' | sort
```

Шаг reduce на bash+awk:

```bash
awk -F' ' 'BEGIN {} { d[$1] += int($2); } END { for (x in d) print(x " " d[x]); }' results/2.txt results/3.txt ...
```

