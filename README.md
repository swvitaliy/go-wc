# MapReduce на go

Map Reduce на go на примере задачи подсчета количества вхождений каждого слова. Во всех реализациях считаем, что слова состоят из символов алфавита и цифр.

Тут представлены 3 функции, реализующие подсчет слов:

1. Однопоточная программа на golang -- [single/main.go](single/main.go)
2. Чтение большого файла в параллель по N горутинам (здесь не учитывается, что слова могут быть "разорваны" в 2 соседних буфера) -- [mapReduceBigFile/main.go](mapReduceBigFile/main.go)
3. Чтение директории рекурсивно в параллель по N горутинам -- [mapReduceDir/main.go](mapReduceDir/main.go)

В последнем примере шаг reduce выполняется параллельно map, за счет чего происходит оптимизация по памяти (не нужно держать в памяти все карты для каждого файла). Так же, за счет этого можно настроить удобное число горутин, уменьшив при этом context switching, и, тем самым, еще дополнительно ускорив программу.
Изменение параметра N от единицы до 5-7 меняет скорость выполнения примерно в 2 раза (видимо, значительная часть времени уходит на дисковые операции чтения/записи). Дальнейшее увеличение количества горутин не приносит увеличения скорости. 

Проверить влияние числа параллельных процессов N на количество контекст свитчей:

```bash
(make build && ./go-wc mrd &> /dev/null &) ; sleep 2 ; cat /proc/$(pgrep go-wc)/status
```

Пример вывода для N=10:

```text
voluntary_ctxt_switches:	124
nonvoluntary_ctxt_switches:	627
```

На моем лаптопе i7-8565U 20GB RAM: 

* при N=3 это соотношение примерно 1 к 10
* при N=5 это соотношение примерно 1 к 3
* при N=10 это соотношение примерно 2-6 к 1 (те. количество принудительно завершенных задач превышает в несколько раз отданных самим процессом)


Для тестирования однострочники на bash:

* Список уникальных слов с частотой (сортировка по частоте):

  ```bash
  cat words.txt | sed -E 's/[^[:alnum:]]/ /g' | tr -s ' ' '\n' | sort | uniq --count | sort -r -n
  ```

* Список уникальных слов с частотой (сортировка по словам):

  ```bash
  cat words.txt | sed -E 's/[^[:alnum:]]/ /g' | tr -s ' ' '\n' | sort | uniq --count | sed -E 's/^\s+//g' | awk '{print $$2 " " $$1; }' | sort
  ```

* Шаг reduce на bash+awk:

  ```bash
  awk -F' ' 'BEGIN {} { d[$1] += int($2); } END { for (x in d) print(x " " d[x]); }' results/2.txt results/3.txt ...
  ```

